{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba830803",
   "metadata": {},
   "source": [
    "# Introduction of camera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecfa778",
   "metadata": {},
   "source": [
    "![title](images/intro0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b6ed84",
   "metadata": {},
   "source": [
    "![title](images/Intrinsic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a83da33",
   "metadata": {},
   "source": [
    "![title](images/Extrinsic1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f8f421",
   "metadata": {},
   "source": [
    "![title](images/distortion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2649b48",
   "metadata": {},
   "source": [
    "# How to convert 2d information to 3d?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db107f6",
   "metadata": {},
   "source": [
    "![title](images/triangulation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a1d64f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bbcff51",
   "metadata": {},
   "source": [
    "# 1. Orgaize file and generate synchronized video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80333c68",
   "metadata": {},
   "source": [
    "## 1.1 Manually organize the collected video files (prototype version) (specific format as shown below, files must be placed exactly as indicated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb77e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def int_to_char(n):\n",
    "    if 0 <= n <= 25:\n",
    "        return chr(ord('A') + n)\n",
    "    else:\n",
    "        raise ValueError(\"Input must be between 0 and 25\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfb56f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/TeamShare/TM_Lab/HHao/CSHL/Demos'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "998df38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'Data'\n",
    "project_names = ['test_data'] #,''\n",
    "number_camera = 4\n",
    "\n",
    "cam_names = [] \n",
    "for i in range(number_camera):\n",
    "    cam_names.append('cam'+int_to_char(i))\n",
    "for p in project_names:\n",
    "    for c in cam_names:\n",
    "        each_path = os.path.join(root_path,os.path.join(p,c))\n",
    "        if not os.path.exists(each_path):\n",
    "            os.makedirs(each_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7aac0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ac89e5d",
   "metadata": {},
   "source": [
    "## 1.2 Synchronize the video files and manually place them in the designated locations (prototype version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "290f0d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.Sync_prototype import synchronize_videos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cda56097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers\n",
      "  built with gcc 7.3.0 (crosstool-NG 1.23.0.449-a04d0)\n",
      "  configuration: --prefix=/tmp/build/80754af9/ffmpeg_1587154242452/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho --cc=/tmp/build/80754af9/ffmpeg_1587154242452/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --enable-avresample --enable-gmp --enable-hardcoded-tables --enable-libfreetype --enable-libvpx --enable-pthreads --enable-libopus --enable-postproc --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --disable-nonfree --enable-gpl --enable-gnutls --disable-openssl --enable-libopenh264 --enable-libx264\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, h264, from 'test.h264':\n",
      "  Duration: N/A, bitrate: N/A\n",
      "    Stream #0:0: Video: h264 (High), yuv420p(progressive), 1200x1080, 30 fps, 30 tbr, 1200k tbn, 60 tbc\n",
      "Input #1, h264, from 'test.h264':\n",
      "  Duration: N/A, bitrate: N/A\n",
      "    Stream #1:0: Video: h264 (High), yuv420p(progressive), 1200x1080, 30 fps, 30 tbr, 1200k tbn, 60 tbc\n",
      "File 'test.mp4' already exists. Overwrite ? [y/N] Not overwriting - exiting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No frame drops detected. No adjustments made.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers\n",
      "  built with gcc 7.3.0 (crosstool-NG 1.23.0.449-a04d0)\n",
      "  configuration: --prefix=/tmp/build/80754af9/ffmpeg_1587154242452/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho --cc=/tmp/build/80754af9/ffmpeg_1587154242452/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --enable-avresample --enable-gmp --enable-hardcoded-tables --enable-libfreetype --enable-libvpx --enable-pthreads --enable-libopus --enable-postproc --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --disable-nonfree --enable-gpl --enable-gnutls --disable-openssl --enable-libopenh264 --enable-libx264\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "[h264 @ 0x63ecba147dc0] Stream #0: not enough frames to estimate rate; consider increasing probesize\n",
      "Input #0, h264, from 'test.h264':\n",
      "  Duration: N/A, bitrate: N/A\n",
      "    Stream #0:0: Video: h264 (High), yuv420p(progressive), 1200x1080, 30 fps, 30 tbr, 1200k tbn, 60 tbc\n",
      "[h264 @ 0x63ecba150340] Stream #0: not enough frames to estimate rate; consider increasing probesize\n",
      "Input #1, h264, from 'test.h264':\n",
      "  Duration: N/A, bitrate: N/A\n",
      "    Stream #1:0: Video: h264 (High), yuv420p(progressive), 1200x1080, 30 fps, 30 tbr, 1200k tbn, 60 tbc\n",
      "File 'test.mp4' already exists. Overwrite ? [y/N] Not overwriting - exiting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No frame drops detected. No adjustments made.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers\n",
      "  built with gcc 7.3.0 (crosstool-NG 1.23.0.449-a04d0)\n",
      "  configuration: --prefix=/tmp/build/80754af9/ffmpeg_1587154242452/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho --cc=/tmp/build/80754af9/ffmpeg_1587154242452/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --enable-avresample --enable-gmp --enable-hardcoded-tables --enable-libfreetype --enable-libvpx --enable-pthreads --enable-libopus --enable-postproc --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --disable-nonfree --enable-gpl --enable-gnutls --disable-openssl --enable-libopenh264 --enable-libx264\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, h264, from 'test.h264':\n",
      "  Duration: N/A, bitrate: N/A\n",
      "    Stream #0:0: Video: h264 (High), yuv420p(progressive), 1200x1080, 30 fps, 30 tbr, 1200k tbn, 60 tbc\n",
      "Input #1, h264, from 'test.h264':\n",
      "  Duration: N/A, bitrate: N/A\n",
      "    Stream #1:0: Video: h264 (High), yuv420p(progressive), 1200x1080, 30 fps, 30 tbr, 1200k tbn, 60 tbc\n",
      "File 'test.mp4' already exists. Overwrite ? [y/N] Not overwriting - exiting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No frame drops detected. No adjustments made.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers\n",
      "  built with gcc 7.3.0 (crosstool-NG 1.23.0.449-a04d0)\n",
      "  configuration: --prefix=/tmp/build/80754af9/ffmpeg_1587154242452/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho --cc=/tmp/build/80754af9/ffmpeg_1587154242452/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --enable-avresample --enable-gmp --enable-hardcoded-tables --enable-libfreetype --enable-libvpx --enable-pthreads --enable-libopus --enable-postproc --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --disable-nonfree --enable-gpl --enable-gnutls --disable-openssl --enable-libopenh264 --enable-libx264\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, h264, from 'test.h264':\n",
      "  Duration: N/A, bitrate: N/A\n",
      "    Stream #0:0: Video: h264 (High), yuv420p(progressive), 1200x1080, 30 fps, 30 tbr, 1200k tbn, 60 tbc\n",
      "Input #1, h264, from 'test.h264':\n",
      "  Duration: N/A, bitrate: N/A\n",
      "    Stream #1:0: Video: h264 (High), yuv420p(progressive), 1200x1080, 30 fps, 30 tbr, 1200k tbn, 60 tbc\n",
      "File 'test.mp4' already exists. Overwrite ? [y/N] Not overwriting - exiting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No frame drops detected. No adjustments made.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33388/3271939956.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msynchronize_videos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcam_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/TeamShare/TM_Lab/HHao/CSHL/Demos/utils/Sync_prototype.py\u001b[0m in \u001b[0;36msynchronize_videos\u001b[0;34m(root_path, project_name, cam_name)\u001b[0m\n\u001b[1;32m    342\u001b[0m                 \u001b[0mselected_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_indices\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_nan_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m                 picked_frames = extract_frames(os.path.join(src_path,'test.mp4'),\n\u001b[0;32m--> 344\u001b[0;31m                                                frame_ind[p][c][selected_indices])\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0mpicked_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpicked_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/TeamShare/TM_Lab/HHao/CSHL/Demos/utils/Sync_prototype.py\u001b[0m in \u001b[0;36mextract_frames\u001b[0;34m(vid_path, frame_list)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mvr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_se2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anipose/lib/python3.7/site-packages/decord/video_reader.py\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CAPI_VideoReaderGetBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbridge_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anipose/lib/python3.7/site-packages/decord/_ffi/_ctypes/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    173\u001b[0m         check_call(_LIB.DECORDFuncCall(\n\u001b[1;32m    174\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             ctypes.byref(ret_val), ctypes.byref(ret_tcode)))\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "synchronize_videos(root_path, project_names,cam_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac2d793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99e75733",
   "metadata": {},
   "source": [
    "# General WorkFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552a2553",
   "metadata": {},
   "source": [
    "![title](images/general_workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab7bd36",
   "metadata": {},
   "source": [
    "# 2. create the XYZ object and training a DeepLabCut to detect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e033fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Extrinsic_check import XYZ_object\n",
    "xyz = XYZ_object(scaling = 1) #set it to 1 if it is about 6cm object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b8d220",
   "metadata": {},
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"images/target.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962d492f",
   "metadata": {},
   "source": [
    "## 3. Train a DeepLabCut model using transfer learning to detect known 3D objects, and manually place them in the corresponding Anipose positions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e1e04",
   "metadata": {},
   "source": [
    "![title](images/synthetic_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079e3a5e",
   "metadata": {},
   "source": [
    "### Work with Gui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e3ef71",
   "metadata": {},
   "source": [
    "![title](images/dlc0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d999c6",
   "metadata": {},
   "source": [
    "![title](images/dlc1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ead933",
   "metadata": {},
   "source": [
    "### or stay with cammand line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1f63fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplabcut\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf2088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_config_path = os.path.join(os.getcwd(),'DeepLabCut/pretrained_xyz_model/config.yaml')\n",
    "pretrained_model_appendix = 'DLC_dlcrnetms5_XYZ_resnet50Aug1shuffle1_900000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_video_path_list = []\n",
    "for project_name in ['xyz','xyz_static',]:\n",
    "    object_video_folder = os.path.join(os.getcwd(), os.path.join('Data',f'4cams/{project_name}'))\n",
    "    \n",
    "    object_video_name_list = [f'{project_name}-camA', \n",
    "                              f'{project_name}-camB', \n",
    "                              f'{project_name}-camC', \n",
    "                              f'{project_name}-camD',]\n",
    "    video_appendix         = '.mp4'\n",
    "    \n",
    "    \n",
    "    for video_name in tqdm.tqdm(object_video_name_list):\n",
    "        video_path = os.path.join(object_video_folder, video_name + video_appendix)\n",
    "        object_video_path_list.append(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c9d024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76850fbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deeplabcut.analyze_videos(pretrained_config_path, \n",
    "                          object_video_path_list, \n",
    "                          save_as_csv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a31d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a416f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the intrinsic parameters if you have one\n",
    "# Load intrinsic calibration result\n",
    "try:\n",
    "    intrinsic_result = np.load(os.path.join(os.path.join(os.getcwd(),'Data/intrinsic',),\n",
    "                                            'intrinsic_params.npy'),allow_pickle=True).item()\n",
    "except:\n",
    "    intrinsic_result = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74692d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Extrinsic_check import Generate_refined_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73d1057",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_iter1_project_path = os.path.join(os.getcwd(),'DeepLabCut/XYZ-fintune_iter1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa4c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for video_path in tqdm.tqdm(object_video_path_list):\n",
    "    prediction_path = os.path.join(video_path[:-4]+pretrained_model_appendix+'.h5' )\n",
    "    if intrinsic_result is not None:\n",
    "        cam_count   = video_name.split('-')[-1]\n",
    "        instrinsicM = intrinsic_result[cam_count]['intr']\n",
    "        dist        = intrinsic_result[cam_count]['dist']\n",
    "    else:\n",
    "        instrinsicM = None\n",
    "        dist        = None\n",
    "    Generate_refined_training_data(prediction_path, video_path, \n",
    "                                   dlc_iter1_project_path,\n",
    "                                   xyz = xyz, obj_scale = 1, obj_unit = 'mm', \n",
    "                                   instrinsicM = instrinsicM, dist     = dist,\n",
    "                                   min_num_kp_fitting = 6,\n",
    "                                   error_threshold    = 5,\n",
    "                                   skip_frames = 10,\n",
    "                                   conf_thres = .4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14f4287",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.add_new_videos(os.path.join(dlc_iter1_project_path,'config.yaml'),\n",
    "                          object_video_path_list, copy_videos=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c94cc7",
   "metadata": {},
   "source": [
    "### Notes: Change this line to your pretraind model path\n",
    "### Our path is Demos/DeepLabCut/XYZ-fintune_iter1/dlc-models/iteration-0/XYZ_resnet50Aug1-trainset99shuffle1/train/pose_cfg.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4959c4d8",
   "metadata": {},
   "source": [
    "![title](images/pretrained_path.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7dbf2",
   "metadata": {},
   "source": [
    "# Now you can go to deeplabcut gui to train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ffc524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4e0758a",
   "metadata": {},
   "source": [
    "### You can do more iteration to improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a3c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dlc_iter2_project_path = os.path.join(os.getcwd(),'DeepLabCut/XYZ-fintune_iter2')\n",
    "dlc_iter3_project_path = os.path.join(os.getcwd(),'DeepLabCut/XYZ-fintune_iter3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6645f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a19a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90e441bf",
   "metadata": {},
   "source": [
    "### For more iterations, repeat previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827ed1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for video_name in tqdm.tqdm(object_video_name_list):\n",
    "    video_path = os.path.join(object_video_folder_iter1, video_name + video_appendix)\n",
    "    prediction_path = os.path.join(object_video_folder , video_name+pretrained_model_appendix+'.h5' )\n",
    "    if intrinsic_result is not None:\n",
    "        cam_count   = video_name.split('-')[-1]\n",
    "        instrinsicM = intrinsic_result[cam_count]['intr']\n",
    "        dist        = intrinsic_result[cam_count]['dist']\n",
    "    else:\n",
    "        instrinsicM = None\n",
    "        dist        = None\n",
    "    Generate_refined_training_data(prediction_path, video_path, \n",
    "                                   dlc_iter2_project_path,\n",
    "                                   xyz = xyz, obj_scale = 1, obj_unit = 'mm', \n",
    "                                   instrinsicM = instrinsicM, dist     = dist,\n",
    "                                   min_num_kp_fitting = 6,\n",
    "                                   error_threshold    = 5,\n",
    "                                   skip_frames = 10,\n",
    "                                   conf_thres = .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ae43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.add_new_videos(os.path.join(dlc_iter2_project_path,'config.yaml'),\n",
    "                          object_video_path_list, copy_videos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0739f0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e59dd3a",
   "metadata": {},
   "source": [
    "# How to use Anipose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0704ee26",
   "metadata": {},
   "source": [
    "![title](images/folder_structure_anipose.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eac86dc",
   "metadata": {},
   "source": [
    "## https://anipose.readthedocs.io/en/latest/tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9157ed4a",
   "metadata": {},
   "source": [
    "## 4. Use Anipose for camera calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c00bd7",
   "metadata": {},
   "source": [
    "![title](images/checkerdescription.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e32608f",
   "metadata": {},
   "source": [
    "conda activate anipose \\\n",
    "anipose calibrate \\\n",
    "anipose filter , if you want to filter it \\\n",
    "anipose triangulate \\\n",
    "anipose label-3d \\\n",
    "anipose label-combined \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80945b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6bfd252",
   "metadata": {},
   "source": [
    "## 6. Check the results of Anipose. If the conditions are met, proceed (good to go). The conditions depend on the actual experiment. If not, go to step 7.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7d03b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Extrinsic_check import XYZ_object, compute_centroid,kabsch_algorithm,\\\n",
    "calculate_3d_error, threshold_confidence,initialize_intr,crop_keypoints\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from utils.Intrinsic_check import iter_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf7bb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read anipose prediction\n",
    "\n",
    "anipose_base_path = os.path.join(os.getcwd()[:-5],'anipose_experiment/')\n",
    "for anipose_p in ['anipose_cylinder_bad_init']: #\n",
    "    for file_p in ['gray.csv','gray_cylinder.csv','gray_static.csv']:\n",
    "        anipose_prediction_path = anipose_base_path+ anipose_p  +'/videos/pose-3d/' + file_p\n",
    "        print(anipose_prediction_path)\n",
    "        try:\n",
    "            prediction = pd.read_csv(anipose_prediction_path)\n",
    "            N_frames = len(prediction)\n",
    "            prediction3d = np.zeros((N_frames,xyz().shape[0],xyz().shape[1]))\n",
    "            for i,j in enumerate(xyz.bodyparts):\n",
    "                prediction3d[:,i,0] =  prediction[f'{j}_x'].values\n",
    "                prediction3d[:,i,1] =  prediction[f'{j}_y'].values\n",
    "                prediction3d[:,i,2] =  prediction[f'{j}_z'].values\n",
    "            error_all = []\n",
    "            for i in range(N_frames):\n",
    "                error_each = calculate_3d_error(xyz(), prediction3d[i])\n",
    "                #print(f\"Mean error: {mean_error}\")\n",
    "                error_all.append(error_each )\n",
    "            error_all = np.array(error_all)\n",
    "            print('Project:',anipose_p, 'Trail:',file_p)\n",
    "            print('median error:',np.round(np.median(error_all),4),'mm')\n",
    "            print('mean error  :'  ,np.round(np.mean(error_all),4)  ,'mm')\n",
    "            print('max error   :'   ,np.round(np.max(error_all) ,4)  ,'mm')\n",
    "            print('min error   :'   ,np.round(np.min(error_all) ,4)  ,'mm')\n",
    "            print('------------------------------------------')\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d691b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2c1fe73",
   "metadata": {},
   "source": [
    "## 7. Perform intrinsic calibration and check the error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad88461",
   "metadata": {},
   "source": [
    "![title](images/checkerdescription.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b013095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Intrinsic_check import CheckerBoard\n",
    "# Define Checkerboard\n",
    "checkerboard = CheckerBoard(12,9,7) # defined as calib.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de308979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2c3cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Intrinsic_check import create_dataset_for_calibration\n",
    "p  = 'intrinsic'\n",
    "for i in range(number_camera):\n",
    "    YOUR_VIDEO_PATH    = os.path.join(root_path,p)\n",
    "    YOUR_VIDEO_NAME    =  f'{p}-{cam_names[i]}.mp4'\n",
    "    create_dataset_for_calibration(YOUR_VIDEO_PATH, YOUR_VIDEO_NAME ,checkerboard, \n",
    "                                   sample_ratio = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29861e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Intrinsic_check import kmean_pick_dataset\n",
    "for i in range(number_camera):\n",
    "    YOUR_VIDEO_PATH    = os.path.join(root_path,p)\n",
    "    YOUR_VIDEO_NAME    =  f'{p}-{cam_names[i]}.mp4'\n",
    "    checker_img_folder = os.path.join(YOUR_VIDEO_PATH,f'{YOUR_VIDEO_NAME[:-4]}_checker_img')\n",
    "    # we choose a total of 128 images from the folder\n",
    "    kmean_pick_dataset(checker_img_folder,numframes2pick = 128) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367dc42d",
   "metadata": {},
   "source": [
    "### Delete the outliers in the folder YOUR_VIDEO_PATH/YOUR_VIDEO_NAME_checker_img/overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e9ca01",
   "metadata": {},
   "source": [
    "![title](images/badchecker2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a72b89d",
   "metadata": {},
   "source": [
    "![title](images/badchecker3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea6cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### it split the checker images into two parts, training and validation.\n",
    "from utils.Intrinsic_check import split_train_test\n",
    "for i in range(number_camera):\n",
    "    YOUR_VIDEO_PATH    = os.path.join(root_path,p)\n",
    "    YOUR_VIDEO_NAME    =  f'{p}-{cam_names[i]}.mp4'\n",
    "    checker_img_folder = os.path.join(YOUR_VIDEO_PATH,f'{YOUR_VIDEO_NAME[:-4]}_checker_img')\n",
    "    split_train_test(checker_img_folder, train_test_ratio = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46df9ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Intrinsic_check import calibrate_kmean_pick_frames\n",
    "from utils.Intrinsic_check import validate_calibrate\n",
    "# load imgpoints generate during creating datasets\n",
    "\n",
    "\n",
    "result = {}\n",
    "for i in range(number_camera):\n",
    "    YOUR_VIDEO_PATH    = os.path.join(root_path,p)\n",
    "    YOUR_VIDEO_NAME    =  f'{p}-{cam_names[i]}.mp4'\n",
    "    checker_img_folder = os.path.join(YOUR_VIDEO_PATH,f'{YOUR_VIDEO_NAME[:-4]}_checker_img')\n",
    "    ret,instrinsicM,dist = calibrate_kmean_pick_frames(checker_img_folder,checkerboard,numframes2pick = 64) \n",
    "    result[cam_names[i]] = {}\n",
    "    result[cam_names[i]]['error'] = ret\n",
    "    result[cam_names[i]]['intr']  = instrinsicM\n",
    "    result[cam_names[i]]['dist']  = dist\n",
    "    \n",
    "    print('training reprojection error',ret,'pixels','\\n',\n",
    "          'intrinsic matrix:','\\n',instrinsicM,'\\n',\n",
    "          'distortion','\\n',np.round(dist,5))\n",
    "    vali_error = validate_calibrate(checker_img_folder,checkerboard,instrinsicM,dist)\n",
    "    print('validation error:', vali_error,'pixels')\n",
    "    result[cam_names[i]]['val_error'] = vali_error\n",
    "    print('-------------')\n",
    "np.save(os.path.join(os.path.join(root_path,p),'intrinsic_params'),result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3c0263",
   "metadata": {},
   "source": [
    "###                   distorted image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f3190c",
   "metadata": {},
   "source": [
    "![title](images/distortion_camB.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7188bc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "intrinsic_result = np.load(os.path.join(os.path.join(os.getcwd(),'Data/intrinsic',),\n",
    "                                            'intrinsic_params.npy'),allow_pickle=True).item()\n",
    "\n",
    "intr_camB = intrinsic_result['camB']['intr'] \n",
    "idst_camB = intrinsic_result['camB']['dist']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f04025",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ee0822",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('images/distortion_camB.png')\n",
    "undistorted_image = cv2.undistort(image, intr_camB, idst_camB)\n",
    "plt.figure(figsize=(12,8),)\n",
    "plt.imshow(undistorted_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0700a55b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a00a7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bc36f1a",
   "metadata": {},
   "source": [
    "## 8. Refine the object detection results and retain the accurate detection points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5568ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Extrinsic_check import refine_dlc_prediction\n",
    "from utils.Extrinsic_check import Generate_refined_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af005bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction_path = '/mnt/TeamShare/TM_Lab/HHao/CSHL/Demos/anipose_experiment/anipose_cylinder_refined/videos/pose-2d/gray_cylinder-camF.h5'\n",
    "#video_path      =  '/mnt/TeamShare/TM_Lab/HHao/CSHL/Demos/anipose_experiment/anipose_cylinder_refined/videos/gray_cylinder-camF.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892a077b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5e75a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1\n",
    "your_xyz_video_name = 'xyz' # as example\n",
    "\n",
    "video_paths      =  [f'DeepLabCut/XYZ-fintune_iter{n_iter}/videos/{your_xyz_video_name}-camA.mp4',\n",
    "                     f'DeepLabCut/XYZ-fintune_iter{n_iter}/videos/{your_xyz_video_name}-camB.mp4',\n",
    "                     f'DeepLabCut/XYZ-fintune_iter{n_iter}/videos/{your_xyz_video_name}-camC.mp4',\n",
    "                     f'DeepLabCut/XYZ-fintune_iter{n_iter}/videos/{your_xyz_video_name}-camD.mp4',]\n",
    "DLC_appdix = 'DLC_resnet50_XYZ_resnet50Aug1shuffle1_80000'\n",
    "for video_path in video_paths:\n",
    "    if intrinsic_result is not None:\n",
    "        cam_count   = video_path.split('-')[-1][:-4]\n",
    "        instrinsicM = intrinsic_result[cam_count]['intr']\n",
    "        dist        = intrinsic_result[cam_count]['dist']\n",
    "    else:\n",
    "        instrinsicM = None\n",
    "        dist  = None\n",
    "    prediction_path = video_path[:-4] + DLC_appdix  + '.h5'\n",
    "    refine_dlc_prediction(video_path,                # Path to the video file\n",
    "                          prediction_path,           # Path to the prediction file\n",
    "                          confident_threshold=0.8,   # Threshold for keypoint confidence (default: 0.6) \n",
    "                          xyz_scaling=1,             # Scaling factor for XYZ coordinates (default: 1,refers to original 6cm object)\n",
    "                          instrinsicM=instrinsicM,          # Intrinsic camera matrix (not used here)\n",
    "                          dist       =dist ,          # Distortion coefficients (not used here)\n",
    "                          error_threshold=3,         # Threshold for allowable error (default: 3) \n",
    "                          min_number_kp_opt = 6,     #  Minimum number of keypoints for optimization (default: 6) \n",
    "                          output_appendix   = 'filtered',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2004ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10c0c659",
   "metadata": {},
   "source": [
    "## 9. Re-estimate the extrinsic parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7754ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.refine_calibration import refine_calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.Extrinsic_check import create_ba_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23a403d-dec8-41df-9ad0-f1c1baf1f2e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "anipose_path = 'anipose_demo/bad_calibrate'\n",
    "intrinsic_result_path = os.path.join(anipose_path ,\n",
    "                                        'intrinsic_params.npy')  \n",
    "video_folder  = 'Data/good_xyz_detection'\n",
    "video_name_list = ['gray_static-camF', 'gray_static-camE','gray_static-camD', \n",
    "                   'gray_static-camC', 'gray_static-camB', 'gray_static-camA',]\n",
    "video_name_list.sort()\n",
    "all_points, all_scores = create_ba_dataset(video_folder,video_name_list)\n",
    "refine_calibration(anipose_path,intrinsic_result_path, all_points, all_scores, file_appendix = '-refine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c4b87",
   "metadata": {},
   "source": [
    "## 11 .Return to step 6 to check the results of Anipose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31243d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir('/mnt/TeamShare/TM_Lab/HHao/CSHL/Demos')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec085020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read anipose prediction\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "anipose_base_path = os.path.join(os.getcwd(),'anipose_demo/')\n",
    "for anipose_p in ['good_calibrate']: #\n",
    "    for file_p in ['gray.csv','gray_cylinder.csv','gray_static.csv']:\n",
    "        anipose_prediction_path = anipose_base_path+ anipose_p  +'/videos/pose-3d/' + file_p\n",
    "        print(anipose_prediction_path)\n",
    "        \n",
    "        prediction = pd.read_csv(anipose_prediction_path)\n",
    "        N_frames = len(prediction)\n",
    "        prediction3d = np.zeros((N_frames,xyz().shape[0],xyz().shape[1]))\n",
    "        for i,j in enumerate(xyz.bodyparts):\n",
    "            prediction3d[:,i,0] =  prediction[f'{j}_x'].values\n",
    "            prediction3d[:,i,1] =  prediction[f'{j}_y'].values\n",
    "            prediction3d[:,i,2] =  prediction[f'{j}_z'].values\n",
    "        error_all = []\n",
    "        for i in range(N_frames):\n",
    "            error_each = calculate_3d_error(xyz(), prediction3d[i])\n",
    "            #print(f\"Mean error: {mean_error}\")\n",
    "            error_all.append(error_each )\n",
    "        error_all = np.array(error_all)\n",
    "        print('Project:',anipose_p, 'Trail:',file_p)\n",
    "        print('median error:',np.round(np.median(error_all),4),'mm')\n",
    "        print('mean error  :'  ,np.round(np.mean(error_all),4)  ,'mm')\n",
    "        print('max error   :'   ,np.round(np.max(error_all) ,4)  ,'mm')\n",
    "        print('min error   :'   ,np.round(np.min(error_all) ,4)  ,'mm')\n",
    "        print('------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d322bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9633a637",
   "metadata": {},
   "source": [
    "# 12. Analysis on 3d keypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23074ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0195fb-e7dd-40ea-a825-d78ec3be8579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e0646",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3d = pd.read_csv('Data/DTL2_2023-07-28_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0a52ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb26e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed01911",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = reducer.fit_transform(data3d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b5387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07755c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP projection of the Demo dataset', fontsize=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1a480c",
   "metadata": {},
   "source": [
    "![title](images/umap.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b8f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "\n",
    "hdb_all = HDBSCAN(cluster_selection_epsilon = 1) # max_cluster_size= 10) #min_cluster_size=None,\n",
    "\n",
    "hdb_all.fit(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e06f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def plot(X, labels, probabilities=None, parameters=None, ground_truth=False, ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(10, 4))\n",
    "    labels = labels if labels is not None else np.ones(X.shape[0])\n",
    "    probabilities = probabilities if probabilities is not None else np.ones(X.shape[0])\n",
    "    # Black removed and is used for noise instead.\n",
    "    unique_labels = set(labels)\n",
    "    colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "    # The probability of a point belonging to its labeled cluster determines\n",
    "    # the size of its marker\n",
    "    proba_map = {idx: probabilities[idx] for idx in range(len(labels))}\n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for noise.\n",
    "            col = [0, 0, 0, 1]\n",
    "\n",
    "        class_index = np.where(labels == k)[0]\n",
    "        for ci in class_index:\n",
    "            ax.plot(\n",
    "                X[ci, 0],\n",
    "                X[ci, 1],\n",
    "                \"x\" if k == -1 else \"o\",\n",
    "                markerfacecolor=tuple(col),\n",
    "                markeredgecolor=\"k\",\n",
    "                markersize=4 if k == -1 else 1 + 5 * proba_map[ci],\n",
    "            )\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    preamble = \"True\" if ground_truth else \"Estimated\"\n",
    "    title = f\"{preamble} number of clusters: {n_clusters_}\"\n",
    "    if parameters is not None:\n",
    "        parameters_str = \", \".join(f\"{k}={v}\" for k, v in parameters.items())\n",
    "        title += f\" | {parameters_str}\"\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b11a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(embedding, labels=hdb_all.labels_, ground_truth=True)\n",
    "plt.xlim(-30, 30) \n",
    "plt.ylim(-30, 30) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c2f8c",
   "metadata": {},
   "source": [
    "![title](images/clustering.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d8ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f80732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512aac91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
